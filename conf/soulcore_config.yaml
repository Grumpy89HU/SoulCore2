# SoulCore 2.0 - Szuverén Entitás Konfiguráció
project:
  name: "SoulCore"
  version: "2.0"
  identity: "Kópé"
  user_lang: "hu"      # A felhasználó nyelve
  internal_lang: "en"  # A rendszer belső nyelve (logika, RAG)
 
active_session:
  user_id: "Grumpy"
  chat_id: "dev_room_001"

api:
  host: "0.0.0.0"       # 0.0.0.0 = minden hálózati kártyán figyel
  port: 8000            # Itt tudod módosítani a portot
  cors_origins: ["*"]   # Biztonsági beállítás a frontendekhez
  timeout: 60           # Válaszadási időkorlát

hardware:
  gpu_count: 1                # Ha megjön a táp, írd át 2-re
  total_vram_limit_mb: 16384  # 16GB VRAM (egy 5060 Ti)
  cuda_device: "cuda:0"

storage:
  model_root: "./models"
  vault_root: "./vault"
  db_limit_gb: 1500           # A bűvös 1.5 TB korlát
  backup_enabled: true

# Slot definíciók - Itt dől el, ki hova kerül
slots:
  translator:
    enabled: true
    model_name: "Translategemma-4b-it"
    model_path: "./models"
    filename: "translategemma-4b-it-Q4_K_M.gguf"
    type: "gguf"
 
  scribe:
    enabled: true
    role: "Gatekeeper"
    engine: "gguf"
    model_name: "Llama-3.2-3B-Instruct"
    model_path: "./models"
    filename: "Llama-3.2-3B-Instruct-Q3_K_XL.gguf"
    gpu_id: 0
    max_vram_mb: 3000
    temperature: 0.1

  valet:
    enabled: true
    role: "Logistics"
    engine: "gguf"
    model_name: "Llama-3.2-3B-Instruct"
    model_path: "./models"
    filename: "Llama-3.2-3B-Instruct-Q3_K_XL.gguf"
    gpu_id: 0
    max_vram_mb: 5000
    temperature: 0.4

  # A Queen és King egyelőre pihen a második kártyáig
  queen:
    enabled: false
    role: "Reasoning"
    engine: "exllamav2"
    model_name: "qwen-3-14b-thinking-exl2"
    gpu_id: 1
    max_vram_mb: 12000

  king:
    enabled: true # Egyelőre False, amíg nincs meg a második kártya/táp
    role: "Sovereign"
    engine: "gguf"
    model_name: "Gemma3 4B"
    model_path: "./models"
    filename: "google_gemma-3-4b-it-Q4_K_M.gguf"
    gpu_id: 0 # Ha bekapcsolod, vigyázz a VRAM-ra!
    max_vram_mb: 14000
    temperature: 0.7

databases:
  vector_vault:
    engine: "qdrant"
    path: "vault/db/soul_vectors" # Átállítva lokálisra a múltkori hiba miatt
  graph_vault:
    engine: "networkx"
    file_path: "./vault/db/social_graph.json"

rag_system:
  enabled: true
  
  # --- Embedding (Gemma-300M) ---
  embedding:
    local_path: "/mnt/raid/soulcore/SoulCore2.0/models/ragsystem/embeddinggemma"
    vector_dimension: 768          # A 300M modell kimeneti dimenziója
    max_input_tokens: 8192          # Ennyit tud egyszerre vektorizálni (nem kell 128k-ig elmenni itt)
    pooling_method: "mean"          # A Gemma embeddingnél ez az ajánlott
    instruction_type:
      query: "query: "              # Kérésnél ezt fűzi elé a jobb találatért
      document: "passage: "         # Indexelésnél ezt használja

  # --- Kontextus és Darabolás (A 128k ereje) ---
  context:
    window_size: 131072             # 128k kontextus támogatás a King/Queen számára
    chunk_size: 4096                # Megemeltük! Mivel 128k a keret, nem kell aprózni.
    chunk_overlap: 512              # Biztonsági átfedés
    max_chunks_per_query: 15        # Akár 15-20 nagy blokkot is betehetünk egyszerre

  # --- Reranker (Qwen3-Reranker-4B) ---
  reranker:
    enabled: false
    local_path: "/mnt/raid/soulcore/SoulCore2.0/models/reranker/qwen3vlreranker2B"
    top_n: 5                        # A 128k miatt több releváns infót is átengedhetünk
    relevance_threshold: 0.65
    
  # --- Keresési Motor (Qdrant beállítások) ---
  retrieval:
    engine: "qdrant"
    distance_metric: "cosine"       # Gemma embeddinghez a Koszinusz-hasonlóság az ideális
    hybrid_search: true             # Szemantikus (vektor) + Szöveges (BM25)
    search_limit: 25                # Ennyit kérünk le az adatbázisból reranking előtt